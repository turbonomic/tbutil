apiVersion: apps/v1
kind: Deployment
metadata:
  name: tbutil-hotwarm
  labels:
    app: tbutil-hotwarm
spec:
  replicas: 1

  selector:
    matchLabels:
      app: tbutil-hotwarm

  template:

    metadata:
      labels:
        app: tbutil-hotwarm

    spec:
      serviceAccountName: tbutil-hotwarm-user

      securityContext:
        fsGroup: 2000

      containers:
      - name: tbutil-hotwarm
        image: turbointegrations/tbutil-hotwarm:2.0i
        imagePullPolicy: IfNotPresent
        env:
          - name: TURBO_MACHINE_ID_FILE
            value: /home/tbutil/.tbutilmachine-id

          - name: TURBO_K8S_SERVICE_NAME
            value: tbutil-hotwarm

          # Set this to true to auto-configure volumes and mounts.
          #
          # Set to false to bye-pass auto-configuration. In this case
          # you will need to uncomment/edit the volumeMounts and volumes
          # sections below and create the tbutil-hotwarm-volume PVC
          # before applying this yaml config.

          - name: TURBO_AUTOCONFIGURE_K8S
            value: "true"

          - name: TZ
            value: GMT

          - name: "SSHNODEPORT"
            value: "31322"
        ports:
          - containerPort: 22


# For trouble-shooting the boostratp, just uncomment these two lines (command
# and args). This causes the boostrap script to be bye-passed so that the pod
# starts and you can "exec" a shell to investigate the problem.

#        command: [ "/bin/sleep" ]
#        args: [ "infinity" ]


# If the auto-creation of volumes doesnt work (eg: not compatible with your cluster
# setup), then you will need to create the tbutil-hotwarm-volume volume by hand and
# configure the "volumeMounts" and "volumes" sections for this deployment. Uncommenting
# the lines below could be a good starting point for the latter step.

#        volumeMounts:
#          - name: tbutil-hotwarm-volume
#            subPath: tbutil
#            mountPath: /home/tbutil
#
#          - name: tbutil-hotwarm-volume
#            subPath: tbexport
#            mountPath: /home/tbexport
#
#          - name: tbutil-hotwarm-volume
#            subPath: etc-ssh
#            mountPath: /etc/ssh
#
#          - name: tbutil-hotwarm-volume
#            subPath: etc-crontabs
#            mountPath: /etc/crontabs
#
#          - name: turbo-volume
#            mountPath: /etc/turbonomic
#
#          - name: topology-processor
#            subPath: helper_dir
#            mountPath: /home/turbonomic/data/helper_dir/topology-processor
#            readOnly: true
#
#          # NB: The "api" and "auth" mounts should only be uncommented if the matching
#          # PVCs already exist in the turbonomic namespace. You can check using:
#          #     kubectl get pvc api -n turbonomic
#          #     kubectl get pvc auth -n turbonomic
#
#          - name: api
#            subPath: helper_dir
#            mountPath: /home/turbonomic/data/helper_dir/api
#            readOnly: true
#
#          - name: auth
#            subPath: helper_dir
#            mountPath: /home/turbonomic/data/helper_dir/auth
#            readOnly: true

#      volumes:
#        - name: tbutil-hotwarm-volume
#          persistentVolumeClaim:
#            claimName: tbutil-hotwarm-volume
#
#        # NB: The confgMap name field below should match the existing config map. You can
#        # determine the needed name by using the command:
#        #     kubectl get configMap -n turbonomic | grep global-properties
#
#        - name: turbo-volume
#          configMap:
#            defaultMode: 420
#            name: global-properties-xl-example
#            optional: true
#
#        - name: topology-processor
#          persistentVolumeClaim:
#            claimName: topology-processor
#
#        # NB: The "api" and "auth" volumes should only be uncommented if the matching
#        # PVCs already exist in the turbonomic namespace. You can check using:
#        #     kubectl get pvc api -n turbonomic
#        #     kubectl get pvc auth -n turbonomic
#
#        - name: api
#          persistentVolumeClaim:
#            claimName: api
#
#        - name: auth
#          persistentVolumeClaim:
#            claimName: auth
#

---

kind: Service
apiVersion: v1
metadata:
  name: tbutil-hotwarm
spec:
  type: NodePort
  ports:
    - name: ssh
      port: 22
      targetPort: 22
      nodePort: 31322

  selector:
    app: tbutil-hotwarm

---

kind: ServiceAccount
apiVersion: v1
metadata:
  name: tbutil-hotwarm-user

---

kind: ClusterRoleBinding

# For OpenShift 3.4-3.7 use apiVersion: v1
# For kubernetes 1.9 use rbac.authorization.k8s.io/v1
# For kubernetes 1.8 use rbac.authorization.k8s.io/v1beta1
apiVersion: rbac.authorization.k8s.io/v1

metadata:
  name: tbutil-hotwarm-binding

subjects:
  - kind: ServiceAccount
    name: tbutil-hotwarm-user
    namespace: turbonomic

roleRef:
  # User creating this resource must have permissions to add this policy to the SA
  kind: ClusterRole
  name: cluster-admin

  # For OpenShift v3.4 remove apiGroup line
  apiGroup: rbac.authorization.k8s.io
